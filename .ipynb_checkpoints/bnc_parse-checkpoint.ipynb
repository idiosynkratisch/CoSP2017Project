{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus = BNCCorpusReader(root='BNC XML/Texts/', fileids=r'[A-K]/\\w*/\\w*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find files containing spoken text\n",
    "spoken = []\n",
    "\n",
    "for text in corpus.fileids():\n",
    "    xml = corpus.xml(text)\n",
    "    if xml.findall('stext') != []:\n",
    "        spoken.append(text)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find subset of files containing spoken text that form the demographically sampled part\n",
    "# indicated by type 'CONVRSN'\n",
    "dem = []\n",
    "\n",
    "for doc in spoken:\n",
    "    xml = corpus.xml(doc)\n",
    "    for text in xml.findall('stext'):\n",
    "        if text.attrib['type'] == 'CONVRSN':\n",
    "            dem.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = defaultdict(list)\n",
    "\n",
    "#find files containing dialogues and compile a list of their indices per file\n",
    "for doc in dem:\n",
    "    xml = corpus.xml(doc)\n",
    "    for conv in xml.iter('div'):\n",
    "        speakers = set()\n",
    "        for utt in conv.iter('u'):\n",
    "            speakers.add(utt.attrib['who'])\n",
    "        if len(speakers) == 2:\n",
    "            dialogues[doc].append(conv.attrib['n'])\n",
    "            \n",
    "#clean up the Robert/Robert2 confusion (see below for an explanation)\n",
    "dialogues['K/KD/KDT.xml'].remove('133401')\n",
    "dialogues['K/KD/KDT.xml'].remove('133501')\n",
    "dialogues['K/KD/KDT.xml'].remove('133502')\n",
    "dialogues['K/KD/KDT.xml'].remove('133602')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K/KB/KB0.xml', 'K/KB/KB1.xml', 'K/KB/KB2.xml', 'K/KB/KB3.xml', 'K/KB/KB4.xml', 'K/KB/KB5.xml', 'K/KB/KB6.xml', 'K/KB/KB7.xml', 'K/KB/KB8.xml', 'K/KB/KB9.xml', 'K/KB/KBA.xml', 'K/KB/KBB.xml', 'K/KB/KBC.xml', 'K/KB/KBD.xml', 'K/KB/KBE.xml', 'K/KB/KBF.xml', 'K/KB/KBG.xml', 'K/KB/KBH.xml', 'K/KB/KBJ.xml', 'K/KB/KBK.xml', 'K/KB/KBL.xml', 'K/KB/KBM.xml', 'K/KB/KBN.xml', 'K/KB/KBP.xml', 'K/KB/KBR.xml', 'K/KB/KBS.xml', 'K/KB/KBT.xml', 'K/KB/KBU.xml', 'K/KB/KBV.xml', 'K/KB/KBW.xml', 'K/KB/KBX.xml', 'K/KB/KBY.xml', 'K/KC/KC0.xml', 'K/KC/KC1.xml', 'K/KC/KC2.xml', 'K/KC/KC3.xml', 'K/KC/KC4.xml', 'K/KC/KC5.xml', 'K/KC/KC6.xml', 'K/KC/KC7.xml', 'K/KC/KC8.xml', 'K/KC/KC9.xml', 'K/KC/KCA.xml', 'K/KC/KCB.xml', 'K/KC/KCC.xml', 'K/KC/KCD.xml', 'K/KC/KCE.xml', 'K/KC/KCF.xml', 'K/KC/KCG.xml', 'K/KC/KCH.xml', 'K/KC/KCJ.xml', 'K/KC/KCK.xml', 'K/KC/KCL.xml', 'K/KC/KCM.xml', 'K/KC/KCN.xml', 'K/KC/KCP.xml', 'K/KC/KCR.xml', 'K/KC/KCS.xml', 'K/KC/KCT.xml', 'K/KC/KCU.xml', 'K/KC/KCV.xml', 'K/KC/KCW.xml', 'K/KC/KCX.xml', 'K/KC/KCY.xml', 'K/KD/KD0.xml', 'K/KD/KD1.xml', 'K/KD/KD2.xml', 'K/KD/KD3.xml', 'K/KD/KD4.xml', 'K/KD/KD5.xml', 'K/KD/KD6.xml', 'K/KD/KD7.xml', 'K/KD/KD8.xml', 'K/KD/KD9.xml', 'K/KD/KDA.xml', 'K/KD/KDB.xml', 'K/KD/KDC.xml', 'K/KD/KDD.xml', 'K/KD/KDE.xml', 'K/KD/KDF.xml', 'K/KD/KDG.xml', 'K/KD/KDH.xml', 'K/KD/KDJ.xml', 'K/KD/KDK.xml', 'K/KD/KDL.xml', 'K/KD/KDM.xml', 'K/KD/KDN.xml', 'K/KD/KDP.xml', 'K/KD/KDR.xml', 'K/KD/KDS.xml', 'K/KD/KDT.xml', 'K/KD/KDU.xml', 'K/KD/KDV.xml', 'K/KD/KDW.xml', 'K/KD/KDX.xml', 'K/KD/KDY.xml', 'K/KE/KE0.xml', 'K/KE/KE1.xml', 'K/KE/KE2.xml', 'K/KE/KE3.xml', 'K/KE/KE4.xml', 'K/KE/KE5.xml', 'K/KE/KE6.xml', 'K/KN/KNR.xml', 'K/KN/KNS.xml', 'K/KN/KNT.xml', 'K/KN/KNU.xml', 'K/KN/KNV.xml', 'K/KN/KNW.xml', 'K/KN/KNY.xml', 'K/KP/KP0.xml', 'K/KP/KP1.xml', 'K/KP/KP2.xml', 'K/KP/KP3.xml', 'K/KP/KP4.xml', 'K/KP/KP5.xml', 'K/KP/KP6.xml', 'K/KP/KP7.xml', 'K/KP/KP8.xml', 'K/KP/KP9.xml', 'K/KP/KPA.xml', 'K/KP/KPB.xml', 'K/KP/KPC.xml', 'K/KP/KPD.xml', 'K/KP/KPE.xml', 'K/KP/KPF.xml', 'K/KP/KPG.xml', 'K/KP/KPH.xml', 'K/KP/KPJ.xml', 'K/KP/KPK.xml', 'K/KP/KPL.xml', 'K/KP/KPM.xml', 'K/KP/KPN.xml', 'K/KP/KPP.xml', 'K/KP/KPR.xml', 'K/KP/KPS.xml', 'K/KP/KPT.xml', 'K/KP/KPU.xml', 'K/KP/KPV.xml', 'K/KP/KPW.xml', 'K/KP/KPX.xml', 'K/KP/KPY.xml', 'K/KR/KR0.xml', 'K/KR/KR1.xml', 'K/KR/KR2.xml', 'K/KS/KSN.xml', 'K/KS/KSP.xml', 'K/KS/KSR.xml', 'K/KS/KSS.xml', 'K/KS/KST.xml', 'K/KS/KSU.xml', 'K/KS/KSV.xml', 'K/KS/KSW.xml']\n"
     ]
    }
   ],
   "source": [
    "print dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDs of the conversations are actually unique, but there appears to have been a mistake confusing two Roberts. Based on the fact that only one of the files (that of Robert2 (PS58H)) lists the recordings these conversations were transcribed from, I'm assuming that he's the person who actually had these conversations, so I'm going to delete them from the other Robert's (PS1CE) list of conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to write metadata and transcripts in format compatible with swda.py\n",
    "\n",
    "from csv import DictWriter\n",
    "\n",
    "\n",
    "def get_data(item, last_shifts=[]):\n",
    "    \"\"\" Constructs text and pos-tag for the item supplied\n",
    "        Arguments:\n",
    "        item (xml.etree.Element): The item to be converted\n",
    "        last_shifts (list of str): description of last <shift> events\n",
    "        \n",
    "        Returns:\n",
    "        text (str): Text for item\n",
    "        pos (str): Pos-tagged version of item\n",
    "    \"\"\"\n",
    "    \n",
    "    if item.tag in {'w', 'c'}:\n",
    "        text = item.text\n",
    "        try:\n",
    "            pos = '{}/{} '.format(item.attrib['hw'], item.attrib['c5'])\n",
    "        except KeyError:\n",
    "            pos = '{}/{} '.format(item.text.rstrip(), item.attrib['c5'])\n",
    "    elif item.tag == 'mw':\n",
    "        #generate text and pos-tag recursively\n",
    "        l = [get_data(it, last_shifts) for it in item]\n",
    "        text = ''.join([i[0] for i in l])\n",
    "        pos = '[{}]/{} '.format(''.join([i[1] for i in l]).rstrip(), item.attrib['c5'])\n",
    "    elif item.tag in {'event', 'vocal'}:\n",
    "        text = '<{}> '.format(item.attrib['desc'])\n",
    "        pos = ''\n",
    "    elif item.tag == 'align':\n",
    "        text = ''\n",
    "        pos = ''\n",
    "    elif item.tag == 'gap':\n",
    "        text = '<<REDACTED ({})>> '.format(item.attrib['desc'])\n",
    "        pos = ''\n",
    "    elif item.tag == 'pause':\n",
    "        text = ''\n",
    "        pos = ''\n",
    "    elif item.tag == 'shift':\n",
    "        if 'new' in item.attrib:\n",
    "            text = '<{}> '.format(item.attrib['new'])\n",
    "            last_shifts.append(item.attrib['new'])\n",
    "        else:\n",
    "            if last_shifts:\n",
    "                text = '</{}> '.format(last_shifts.pop())\n",
    "            else:\n",
    "                text = ''\n",
    "        pos = ''\n",
    "    elif item.tag == 'unclear':\n",
    "        if item.text == None:\n",
    "            text = '(()) '\n",
    "        else:\n",
    "            text = '(({})) '.format(item.text)\n",
    "        pos = ''\n",
    "    elif item.tag == 'trunc':\n",
    "        # generate text and pos-tag recursively\n",
    "        l = [get_data(it, last_shifts) for it in item]\n",
    "        # add '-' at the end of the truncated text\n",
    "        text = ''.join([i[0] for i in l]).rstrip() + '- '\n",
    "        # concatenate available pos-tags\n",
    "        pos = ''.join([i[1] for i in l])\n",
    "    else:\n",
    "        print 'Unrecognized tag: {}'.format(item.tag)\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "    return text, pos\n",
    "\n",
    "def write_transcript(conv, doc, directory, pos=False):\n",
    "    \"\"\" Writes a transcript of the conversation in csv-format and\n",
    "        returns filename and ids of callers A and B\n",
    "    \n",
    "        Argument:\n",
    "        conv (xml.etree.Element): The conversation to be transcribed.\n",
    "        doc (str): The file containing the conversation\n",
    "        directory (str): Directory to be written to\n",
    "        pos (boolean): Transcribe BNC/CLAWS POS-tags?\n",
    "    \"\"\"\n",
    "    filename = '{}/{}'.format(directory, conv.attrib['n'] + '.csv')\n",
    "    header = ['swda_filename',       #original BNC XML file\n",
    "              'ptb_basename',        #id of the recording\n",
    "              'conversation_no',     #id of the div\n",
    "              'transcript_index',    #index of subutterance (corresponds to <s>) in the transcript\n",
    "              'act_tag',             #unused\n",
    "              'caller',              #A (first speaker) or B (second speaker)\n",
    "              'turn_index',          #index of utterance (corresponds to <u> or top-level <unclear>) in the transcript\n",
    "              'subutterance_index',  #index of subutterance within utterance (<s> within <u>)\n",
    "              'text',                #text of subutterance (<s>)\n",
    "              'pos',                 #POS-tags of subutterance\n",
    "              'trees',               #field for trees to be added by the Stanford Parser\n",
    "             ]\n",
    "    \n",
    "    n = conv.attrib['n']\n",
    "    #try finding recording and setting id as attributes of the conversation\n",
    "    try:\n",
    "        recording, setting = conv.attrib['decls'].split()[:2]\n",
    "    except KeyError:\n",
    "        # if this fails try finding the corresponding recording and setting 'manually'\n",
    "        xml = corpus.xml(doc)\n",
    "        rec = xml.find(\".//recording[@n='{}']\".format(n))\n",
    "        se = xml.find(\".//setting[@n='{}']\".format(n))\n",
    "        if rec == None:\n",
    "            print 'Failed to find recording ID, skipping'\n",
    "            return None\n",
    "        else:\n",
    "            recording = rec.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "        if se == None:\n",
    "            print 'Failed to find setting ID, skipping'\n",
    "            return None\n",
    "        else:\n",
    "            setting = se.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "    \n",
    "    with open(filename, 'w+') as f:\n",
    "        writer = DictWriter(f, header)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # variables for ids of first and second speaker\n",
    "        A = None\n",
    "        B = None\n",
    "        transcript_index = 0\n",
    "        turn_index = 0\n",
    "        \n",
    "        for utt in conv.findall('*[@who]'):\n",
    "            #set values that are the same for all utterances in a conversation\n",
    "            d = {'swda_filename': doc,\n",
    "                 'ptb_basename': recording,\n",
    "                 'conversation_no': n}\n",
    "            #update and write turn_index, reset subutterance_index\n",
    "            turn_index += 1\n",
    "            subutterance_index = 1\n",
    "            d['turn_index'] = turn_index\n",
    "            # find out who is speaking\n",
    "            if utt.attrib['who'] == A:\n",
    "                d['caller'] = 'A'\n",
    "            elif utt.attrib['who'] == B:\n",
    "                d['caller'] = 'B'\n",
    "            elif A == None:\n",
    "                A = utt.attrib['who']\n",
    "                d['caller'] = 'A'\n",
    "            elif B == None:\n",
    "                B = utt.attrib['who']\n",
    "                d['caller'] = 'B'\n",
    "            else:\n",
    "                print 'Something went wrong: Could not identify speaker'\n",
    "                raise KeyboardInterrupt\n",
    "            #check if we have a top-level unclear element\n",
    "            if utt.tag == 'unclear':\n",
    "                #if so, write '(())' as text and continue\n",
    "                d['transcript_index'] = transcript_index\n",
    "                d['subutterance_index'] = subutterance_index\n",
    "                d['text'] = '(())'\n",
    "                writer.writerow(d)\n",
    "                transcript_index += 1\n",
    "                continue\n",
    "            # intialize stack for shifts in vocal quality\n",
    "            last_shifts = []\n",
    "            for subutt in utt:\n",
    "                if subutt.tag == 's':\n",
    "                    # generate text and pos-tag recursively\n",
    "                    l = [get_data(it, last_shifts) for it in subutt]\n",
    "                    # concatenate text\n",
    "                    text = ''.join([i[0] for i in l]).rstrip()\n",
    "                    # concatenate available pos-tags\n",
    "                    pos = ''.join([i[1] for i in l]).rstrip()\n",
    "                else:\n",
    "                    text, pos = get_data(subutt, last_shifts)\n",
    "                d['transcript_index'] = transcript_index\n",
    "                d['subutterance_index'] = subutterance_index\n",
    "                d['text'] = text\n",
    "                d['pos'] = pos\n",
    "                writer.writerow(d)\n",
    "                transcript_index += 1\n",
    "                subutterance_index += 1\n",
    "    \n",
    "    return n, A, B, recording, setting\n",
    "    \n",
    "    \n",
    "def write_metadata(doc, ids, writer):\n",
    "    \"\"\" Compiles metadata for conv and writes it using writer\n",
    "    \n",
    "        Argument:\n",
    "        doc (xml.etree.Element): The XML-tree containing the conversation for which the metadata is to be written\n",
    "        ids (iterable): Contains ids for conversation, caller A and B, as well as recording and setting IDs\n",
    "        writer(csv.DictWriter): DictWriter object to the metadata file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper dicts to translate from BNC metadata to SWBD-like metadata\n",
    "    sex = {'m': 'MALE', 'f': 'FEMALE', 'u': 'UNKNOWN'}\n",
    "    # since exact birth dates are not always available in BNC, we choose the midpoint of each\n",
    "    # age group and substract it from the date of recording to obtain an estimate, if possible.\n",
    "    age_groups = {'Ag0': 10, 'Ag1': 20, 'Ag2': 30, 'Ag3': 40, 'Ag4': 52, 'Ag5': 75}\n",
    "    dialect_areas = {'CAN': 'Canadian',\n",
    "                     'NONE': '',\n",
    "                     'XDE': 'German',\n",
    "                     'XEA': 'East Anglian',\n",
    "                     'XFR': 'French',\n",
    "                     'XHC': 'Home Counties',\n",
    "                     'XHM': 'Humberside',\n",
    "                     'XIR': 'Irish',\n",
    "                     'XIS': 'Indian subcontinent',\n",
    "                     'XLC': 'Lancashire',\n",
    "                     'XLO': 'London',\n",
    "                     'XMC': 'Central Midlands',\n",
    "                     'XMD': 'Merseyside',\n",
    "                     'XME': 'North-east Midlands',\n",
    "                     'XMI': 'Midlands',\n",
    "                     'XMS': 'South Midlands',\n",
    "                     'XMW': 'North-west Midlands',\n",
    "                     'XNC': 'Central Northern England',\n",
    "                     'XNE': 'North-east England',\n",
    "                     'XNO': 'Nothern England',\n",
    "                     'XOT': '',\n",
    "                     'XSD': 'Scottish',\n",
    "                     'XSL': 'Lower south-west England',\n",
    "                     'XSS': 'Central south-west England',\n",
    "                     'XSU': 'Upper south-west England',\n",
    "                     'XUR': 'European',\n",
    "                     'XUS': 'American (US)',\n",
    "                     'XWA': 'Welsh',\n",
    "                     'XWE': 'West Indian'}\n",
    "    education = {'Ed1': 0, 'X':9}\n",
    "    \n",
    "    n, A, B, rec_id, set_id = ids\n",
    "    \n",
    "    #fill the rows that can already be filled\n",
    "    d = {'conversation_no': n, 'from_caller_id': A, 'to_caller_id': B}\n",
    "    \n",
    "    recording = doc.find(\".//recording[@n='{}']\".format(n))\n",
    "    date = None\n",
    "    try:\n",
    "        date = recording.attrib['date'].split('-')\n",
    "        # format date in the swda way\n",
    "        d['talk_day'] = recording.attrib['date'].replace('-','')[2:]\n",
    "    except KeyError:\n",
    "        # swda_time expects this to be set, so we set it to '010101' if undefined\n",
    "        d['talk_day'] = '010101'\n",
    "    \n",
    "    try:\n",
    "        d['length'] = recording.attrib['dur']\n",
    "    except KeyError:\n",
    "        # swda_times expects this to be set, so we set it to '0' if undefined\n",
    "        d['length'] = 0\n",
    "    \n",
    "    setting = doc.find(\".//setting[@n='{}']\".format(n))\n",
    "    try:\n",
    "        d['topic_description'] = setting.find(\"activity\").text\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    id_str = '@{http://www.w3.org/XML/1998/namespace}id'\n",
    "    \n",
    "    A = doc.find(\".//person[{}='{}']\".format(id_str, A))\n",
    "    B = doc.find(\".//person[{}='{}']\".format(id_str, B))\n",
    "    \n",
    "    files = {'A': {}, 'B': {}}\n",
    "    \n",
    "    # try finding values for personal information, preferring the more explicit information\n",
    "    for person in files:\n",
    "        for att in ['sex', 'age', 'educ', 'dialect']:\n",
    "            value = eval(person).find(att)\n",
    "            if value:\n",
    "                files[person][att] = value.text\n",
    "                print value.text\n",
    "            else:\n",
    "                try:\n",
    "                    files[person][att] = eval(person).attrib[att]\n",
    "                except KeyError:\n",
    "                    files[person][att] = ''\n",
    "        if files[person]['age'] == '':\n",
    "            try:\n",
    "                ageGroup = eval(person).attrib['ageGroup']\n",
    "                files[person]['age'] = age_groups[ageGroup]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        # compute birth_year if possible\n",
    "        if files[person]['age'] != '' and date:\n",
    "            files[person]['birth_year'] = int(date[0]) - int(files[person]['age'])\n",
    "        else:\n",
    "            # swda_time expects birth years to be set, so we set it to 1789\n",
    "            files[person]['birth_year'] = 1789\n",
    "        # convert dialect area if necessary\n",
    "        dialect = files[person]['dialect']\n",
    "        if dialect in dialect_areas:\n",
    "            files[person]['dialect'] = dialect_areas[dialect]\n",
    "        # set dialect area to 'UNK' if unknown\n",
    "        if dialect == '':\n",
    "            files[person]['dialect'] = 'UNK'\n",
    "        # convert education level if possible, else set to unknown (i.e. '9')\n",
    "        educ = files[person]['educ']\n",
    "        if educ in education:\n",
    "            files[person]['educ'] = education['educ']\n",
    "        else:\n",
    "            files[person]['educ'] = 9\n",
    "        # convert sex to swda format\n",
    "        bnc_sex = files[person]['sex']\n",
    "        if bnc_sex in sex:\n",
    "            files[person]['sex'] = sex[bnc_sex]\n",
    "        else:\n",
    "            files[person]['sex'] = 'UNKNOWN'\n",
    "            \n",
    "    \n",
    "    # use found personal data to fill corresponding field in the row\n",
    "    d['from_caller_sex'] = files['A']['sex']\n",
    "    d['from_caller_education'] = files['A']['educ']\n",
    "    d['from_caller_birth_year'] = files['A']['birth_year']\n",
    "    d['from_caller_dialect_area'] = files['A']['dialect']\n",
    "    \n",
    "    d['to_caller_sex'] = files['B']['sex']\n",
    "    d['to_caller_education'] = files['B']['educ']\n",
    "    d['to_caller_birth_year'] = files['B']['birth_year']\n",
    "    d['to_caller_dialect_area'] = files['B']['dialect']\n",
    "    \n",
    "    # write compiled row to the metadata file\n",
    "    writer.writerow(d)\n",
    "            \n",
    "    \n",
    "def convert_bnc(dialogues, directory='bnc', pos=False):\n",
    "    \"\"\" Iterates over fileids and converts all contained conversations to csv-files\n",
    "        writing metadata in the process\n",
    "        \n",
    "        Arguments:\n",
    "        dialogues(dict of [str, list of str]):\n",
    "            Dictionary indexed by relative paths of files containing\n",
    "            conversations to be transcribed, with value a list of ids\n",
    "            (<div:n>) of those conversations\n",
    "        directory: Relative path of the directory to be written to\n",
    "        pos: Use POS-tags from BNC?\n",
    "    \"\"\"\n",
    "    metadata_header = ['conversation_no',\n",
    "                       'talk_day',\n",
    "                       'length',\n",
    "                       'topic_description',\n",
    "                       'prompt',\n",
    "                       'from_caller_sex',\n",
    "                       'from_caller_education',\n",
    "                       'from_caller_birth_year',\n",
    "                       'from_caller_dialect_area',\n",
    "                       'to_caller_sex',\n",
    "                       'to_caller_education',\n",
    "                       'to_caller_birth_year',\n",
    "                       'to_caller_dialect_area',\n",
    "                       'from_caller_id',\n",
    "                       'to_caller_id'\n",
    "                        ]\n",
    "    \n",
    "    metadata_filename = '{}/bnc-metadata.csv'.format(directory)\n",
    "    \n",
    "    with open(metadata_filename, 'w+') as metadata_file:\n",
    "        meta_writer = DictWriter(metadata_file, metadata_header)\n",
    "        meta_writer.writeheader()\n",
    "        \n",
    "        for doc in dialogues:\n",
    "            doc_xml = corpus.xml(doc)\n",
    "            for conv_id in dialogues[doc]:\n",
    "                conv = doc_xml.find(\".//div[@n='{}']\".format(conv_id))\n",
    "                ids = write_transcript(conv, doc, directory, pos)\n",
    "                write_metadata(doc_xml, ids, meta_writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K/KB/KBB.xml\n",
      "049501\n",
      "<Element 'stext' at 0x1a1dfe6c90>\n"
     ]
    }
   ],
   "source": [
    "doc = dialogues.keys()[25]\n",
    "\n",
    "print doc\n",
    "\n",
    "doc_xml = corpus.xml(doc)\n",
    "stext = doc_xml.find('stext')\n",
    "print dialogues[doc][5]\n",
    "print stext\n",
    "\n",
    "for div in stext.findall('div'):\n",
    "    if div.attrib['n'] == dialogues[doc][0]:\n",
    "        print 'Found it!'\n",
    "        conv = div\n",
    "    print div.attrib['n']\n",
    "\n",
    "f = open('bnc_test/bnc-metadata.csv', 'w+')    \n",
    "meta_writer = DictWriter()\n",
    "\n",
    "metadata_header = ['conversation_no',\n",
    "                       'talk_day',\n",
    "                       'length',\n",
    "                       'topic_description',\n",
    "                       'prompt',\n",
    "                       'from_caller_sex',\n",
    "                       'from_caller_education',\n",
    "                       'from_caller_birth_year',\n",
    "                       'from_caller_dialect_area',\n",
    "                       'to_caller_sex',\n",
    "                       'to_caller_education',\n",
    "                       'to_caller_birth_year',\n",
    "                       'to_caller_dialect_area',\n",
    "                       'from_caller_id',\n",
    "                       'to_caller_id'\n",
    "                        ]\n",
    "\n",
    "ids = write_transcript(conv, doc, 'bnc_test', pos=True)\n",
    "write_metadata(doc_xml, ids, f)\n",
    "\n",
    "#convert_bnc(dialogues, 'bnc_test', pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "transcript 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<swda_time.Transcript instance at 0x1a0bdeac20>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from swda_time import CorpusReader\n",
    "\n",
    "bnc_corpus = CorpusReader('bnc_test', 'bnc_test/bnc-metadata.csv')\n",
    "\n",
    "bnc_corpus.iter_transcripts().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
